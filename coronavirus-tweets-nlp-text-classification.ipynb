{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1472453,"sourceType":"datasetVersion","datasetId":863934}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-13T12:39:04.986726Z","iopub.execute_input":"2024-04-13T12:39:04.98743Z","iopub.status.idle":"2024-04-13T12:39:06.055783Z","shell.execute_reply.started":"2024-04-13T12:39:04.987372Z","shell.execute_reply":"2024-04-13T12:39:06.054514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We have a dataset that contains tweets from people from different countries at the time of the Covid-19 crisis, classified into several categories, such as negative, positive, and neutral. We will analyze and then train a model to make predictions.","metadata":{}},{"cell_type":"markdown","source":"###  السلام عليكم لدينا مجموعة بيانات تحتوي علي تغريدات لاشخاص من دول مختلفة في وقت ازمة كوفيد19 مصنفعة لعدة تصنيفات مثل سلبي وايجابي ومحايد سنوم بتحليل ثم تدريب نموذج علي  التنبأ ","metadata":{}},{"cell_type":"code","source":"# استردراد المكتبات الضرورية \n#import  necessary libraries\n# استيراد مكتبة باندا\nimport pandas as pd\n# Importing the pandas library\n# استيراد مكتبة نمباي\nimport numpy as np\n# Importing the numpy library\n# استيراد وظيفة word_tokenize من مكتبة NLTK\nfrom nltk import word_tokenize\n# Importing the word_tokenize function from the NLTK library\n# استيراد مكتبة stopwords من مكتبة NLTK\nfrom nltk.corpus import stopwords\n# Importing the stopwords library from the NLTK library\n# استيراد مكتبة spacy\nimport spacy \n# Importing the spacy library\n# تحميل موديل اللغة الإنجليزية من مكتبة spacy\nnlp = spacy.load(\"en_core_web_sm\") \n# Loading the English language model from the spacy library\n# تجاهل الإنذارات\nimport warnings\nwarnings.filterwarnings('ignore')\n# Ignoring warnings\n# استيراد مكتبة re\nimport re\n# Importing the re library\n# استيراد مكتبة string\nimport string\n# Importing the string library\nimport nltk\n# تنزيل قائمة الكلمات الوقفية من مكتبة NLTK\n\n# Downloading the list of stopwords from the NLTK library\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords          \nfrom nltk.tokenize import TweetTokenizer\n\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n#في العربية: هذه المكتبة توفر وظيفة لتقسيم البيانات إلى مجموعات التدريب والاختبار\n# This library provides a function for splitting the data into training and testing sets.\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Flatten,Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:11:47.754019Z","iopub.execute_input":"2024-04-13T13:11:47.754815Z","iopub.status.idle":"2024-04-13T13:11:48.957392Z","shell.execute_reply.started":"2024-04-13T13:11:47.754779Z","shell.execute_reply":"2024-04-13T13:11:48.956546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# قراءة مجموعة البيانات الاختبارية من المسار المحدد\n# Reading the test dataset from the specified path\ndf = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv')\n\n# قراءة مجموعة البيانات التدريبية من المسار المحدد مع ترميز 'ISO-8859-1'\n# Reading the train dataset from the specified path with 'ISO-8859-1' encoding\ndf1 = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='ISO-8859-1')\n\n# عرض إطار البيانات\n# Displaying the DataFrame\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:38.888287Z","iopub.execute_input":"2024-04-13T12:57:38.88863Z","iopub.status.idle":"2024-04-13T12:57:39.320519Z","shell.execute_reply.started":"2024-04-13T12:57:38.888604Z","shell.execute_reply":"2024-04-13T12:57:39.319241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:40.551896Z","iopub.execute_input":"2024-04-13T12:57:40.552279Z","iopub.status.idle":"2024-04-13T12:57:40.567375Z","shell.execute_reply.started":"2024-04-13T12:57:40.552248Z","shell.execute_reply":"2024-04-13T12:57:40.566254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We analyze and explore data In this process, we will work on df1 because the two files have the same data and columns, with only different numbers\n","metadata":{}},{"cell_type":"code","source":"# استخدام الدالة info() لعرض معلومات عن إطار البيانات df1\n# Using the info() function to display information about the DataFrame df1\ndf1.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:42.555807Z","iopub.execute_input":"2024-04-13T12:57:42.556719Z","iopub.status.idle":"2024-04-13T12:57:42.596978Z","shell.execute_reply.started":"2024-04-13T12:57:42.556674Z","shell.execute_reply":"2024-04-13T12:57:42.596007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# استخدام الدالة describe() لعرض إحصائيات وصفية عن إطار البيانات df1\n# Using the describe() function to display descriptive statistics about the DataFrame df1\ndf1.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:43.835171Z","iopub.execute_input":"2024-04-13T12:57:43.835505Z","iopub.status.idle":"2024-04-13T12:57:43.867001Z","shell.execute_reply.started":"2024-04-13T12:57:43.835478Z","shell.execute_reply":"2024-04-13T12:57:43.866007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.Sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:45.165575Z","iopub.execute_input":"2024-04-13T12:57:45.165954Z","iopub.status.idle":"2024-04-13T12:57:45.180214Z","shell.execute_reply.started":"2024-04-13T12:57:45.165923Z","shell.execute_reply":"2024-04-13T12:57:45.179136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.TweetAt.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:23:03.471254Z","iopub.execute_input":"2024-04-13T01:23:03.471685Z","iopub.status.idle":"2024-04-13T01:23:03.489658Z","shell.execute_reply.started":"2024-04-13T01:23:03.471651Z","shell.execute_reply":"2024-04-13T01:23:03.488262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will now process the empty values ​​and then visualize the data","metadata":{}},{"cell_type":"code","source":"df['Location'] = df['Location'].str.replace('Israel ??', '')\ndf1['Location'] = df1['Location'].str.replace('Israel ??', '')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:48.264229Z","iopub.execute_input":"2024-04-13T12:57:48.264614Z","iopub.status.idle":"2024-04-13T12:57:48.288472Z","shell.execute_reply.started":"2024-04-13T12:57:48.264578Z","shell.execute_reply":"2024-04-13T12:57:48.287508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# استخدام الدالة isnull() لتحديد القيم الناقصة في إطار البيانات df1\ndf1.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:49.820818Z","iopub.execute_input":"2024-04-13T12:57:49.821171Z","iopub.status.idle":"2024-04-13T12:57:49.848575Z","shell.execute_reply.started":"2024-04-13T12:57:49.821144Z","shell.execute_reply":"2024-04-13T12:57:49.847508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# استخدام الدالة isnull() لتحديد القيم الناقصة في إطار البيانات df\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:50.850033Z","iopub.execute_input":"2024-04-13T12:57:50.850395Z","iopub.status.idle":"2024-04-13T12:57:50.861465Z","shell.execute_reply.started":"2024-04-13T12:57:50.850366Z","shell.execute_reply":"2024-04-13T12:57:50.860247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# استخدام الدالة dropna() لحذف الصفوف التي تحتوي على قيم مفقودة في إطار البيانات df1\ndf.dropna(inplace=True)\ndf1.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:52.020266Z","iopub.execute_input":"2024-04-13T12:57:52.020754Z","iopub.status.idle":"2024-04-13T12:57:52.052174Z","shell.execute_reply.started":"2024-04-13T12:57:52.020713Z","shell.execute_reply":"2024-04-13T12:57:52.051279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values = df1['Sentiment'].unique()\ncounts = df1['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:55.576211Z","iopub.execute_input":"2024-04-13T12:57:55.579004Z","iopub.status.idle":"2024-04-13T12:57:55.592454Z","shell.execute_reply.started":"2024-04-13T12:57:55.578961Z","shell.execute_reply":"2024-04-13T12:57:55.591287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.barplot(x=values, y=counts)\nplt.xlabel('Values')\nplt.ylabel('Counts')\nplt.title('Bar Chart of Sentiment')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:23:15.585316Z","iopub.execute_input":"2024-04-13T01:23:15.586348Z","iopub.status.idle":"2024-04-13T01:23:15.911581Z","shell.execute_reply.started":"2024-04-13T01:23:15.586268Z","shell.execute_reply":"2024-04-13T01:23:15.91026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))  # تحديد عرض وارتفاع التصوير بالبوصة\nplt.pie(counts, labels=values, autopct='%1.1f%%')\nplt.title('Pie Chart of Sentiment')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:23:18.852855Z","iopub.execute_input":"2024-04-13T01:23:18.853227Z","iopub.status.idle":"2024-04-13T01:23:19.083779Z","shell.execute_reply.started":"2024-04-13T01:23:18.853197Z","shell.execute_reply":"2024-04-13T01:23:19.082952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We find here that the Neutral ratio is the most among all and the least is Extremely Negative ","metadata":{}},{"cell_type":"code","source":"dates = df1['TweetAt'].unique()\nvalues1 = df1['TweetAt'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:57:59.873888Z","iopub.execute_input":"2024-04-13T12:57:59.874315Z","iopub.status.idle":"2024-04-13T12:57:59.887432Z","shell.execute_reply.started":"2024-04-13T12:57:59.874281Z","shell.execute_reply":"2024-04-13T12:57:59.886232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = pd.to_datetime(dates)\n\n# إنشاء مخطط خطي\nplt.figure(figsize=(12, 6))\nplt.plot(dates, values1, marker='o')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.title('Line Plot of Values over Time')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:58:56.324521Z","iopub.execute_input":"2024-04-13T12:58:56.325229Z","iopub.status.idle":"2024-04-13T12:58:56.695473Z","shell.execute_reply.started":"2024-04-13T12:58:56.325194Z","shell.execute_reply":"2024-04-13T12:58:56.694687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This chart shows us that tweets related to Covid-19 were high in the beginning, but as we got used to it, the number of tweets decreased.\n","metadata":{}},{"cell_type":"code","source":"# إنشاء مخطط عمودي\nplt.figure(figsize=(40, 30))\nplt.bar(dates, values1, color='green')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.title('Bar Plot of Values over Time')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:23:27.371142Z","iopub.execute_input":"2024-04-13T01:23:27.371533Z","iopub.status.idle":"2024-04-13T01:23:28.327159Z","shell.execute_reply.started":"2024-04-13T01:23:27.371503Z","shell.execute_reply":"2024-04-13T01:23:28.326253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here the number for each day is explained in detail\n\n","metadata":{}},{"cell_type":"code","source":"#In searching the Places column, we find that we have 1717 different places\n#في البحث في عامود الاماكن نجد ان لدينا 1717 مكان مختلف \ndf.Location.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:04.58957Z","iopub.execute_input":"2024-04-13T12:59:04.590277Z","iopub.status.idle":"2024-04-13T12:59:04.602737Z","shell.execute_reply.started":"2024-04-13T12:59:04.590246Z","shell.execute_reply":"2024-04-13T12:59:04.60142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# إنشاء مخطط الانتشار\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='Sentiment', y='Location', data=df[20:60])\nplt.xlabel('Column 1')\nplt.ylabel('Column 2')\nplt.title('Scatter Plot of Column 1 vs Column 2')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:06.363074Z","iopub.execute_input":"2024-04-13T12:59:06.363604Z","iopub.status.idle":"2024-04-13T12:59:06.943231Z","shell.execute_reply.started":"2024-04-13T12:59:06.363575Z","shell.execute_reply":"2024-04-13T12:59:06.942373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With the creation of the chart, we find that there are invalid and almost imaginary values ​​that must be eliminated\n### مع انشاء المخطط نجد ان هناك قيم غير صالحه وشبه وهمية يجب التخلص منها \n","metadata":{}},{"cell_type":"code","source":"print(df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:10.06461Z","iopub.execute_input":"2024-04-13T12:59:10.065684Z","iopub.status.idle":"2024-04-13T12:59:10.070936Z","shell.execute_reply.started":"2024-04-13T12:59:10.065634Z","shell.execute_reply":"2024-04-13T12:59:10.069998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First, we will convert the entire column to text because there are some values ​​in the middle that are not text\n# اولا سنقوم بتحويل العامود بالكامل الي نص لان هناك بعض القيم في الوسط ليست نصوص\ndf['Location'] = df['Location'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:10.947672Z","iopub.execute_input":"2024-04-13T12:59:10.948069Z","iopub.status.idle":"2024-04-13T12:59:10.953747Z","shell.execute_reply.started":"2024-04-13T12:59:10.948037Z","shell.execute_reply":"2024-04-13T12:59:10.95256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# قراءة البيانات إلى DataFrame\n# على سبيل المثال، لنفترض أن لديك DataFrame يسمى df والعمود الذي تريد تنظيفه هو \"Location\"\n\n# دالة لإزالة الرموز غير المرغوب فيها باستخدام مكتبة re\ndef remove_special_characters(text):\n    # تعريف التعبير العادي لإزالة الرموز\n    pattern = r'[^a-zA-Z\\s]'   # يتم الاحتفاظ بالأحرف الأبجدية الإنجليزية، الأرقام، والفراغات\n \n    # استخدام الدالة sub() لإزالة الرموز باستخدام التعبير العادي\n    cleaned_text = re.sub(pattern, '', text)\n\n    return cleaned_text\n\n# تطبيق الدالة على عمود \"Location\"\ndf['Location'] = df['Location'].apply(remove_special_characters)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:12.58615Z","iopub.execute_input":"2024-04-13T12:59:12.587174Z","iopub.status.idle":"2024-04-13T12:59:12.603118Z","shell.execute_reply.started":"2024-04-13T12:59:12.587136Z","shell.execute_reply":"2024-04-13T12:59:12.601741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# إنشاء مخطط الانتشار\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='Sentiment', y='Location', data=df[20:60])\nplt.xlabel('Column 1')\nplt.ylabel('Column 2')\nplt.title('Scatter Plot of Column 1 vs Column 2')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T02:31:27.872537Z","iopub.execute_input":"2024-04-13T02:31:27.873564Z","iopub.status.idle":"2024-04-13T02:31:28.453714Z","shell.execute_reply.started":"2024-04-13T02:31:27.873528Z","shell.execute_reply":"2024-04-13T02:31:28.452331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The data was set and the visualization was successful\n","metadata":{}},{"cell_type":"markdown","source":"### Now, to make it easier to visualize the Sentiment column better, we will convert it to numbers\n### الان ليسهل تصور عامود Sentiment بشكل افضل سنقوم بتحويله الي ارقام","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.hist(df1['Sentiment'], bins=10)\nplt.xlabel('Sentiment')\nplt.ylabel('Frequency')\nplt.title('Histogram of Sentiment')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:23:50.966677Z","iopub.execute_input":"2024-04-13T01:23:50.967746Z","iopub.status.idle":"2024-04-13T01:23:51.256957Z","shell.execute_reply.started":"2024-04-13T01:23:50.967697Z","shell.execute_reply":"2024-04-13T01:23:51.25576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# قائمة بالتطابق بين القيم النصية والقيم العددية المقابلة\nmapping = {\n    'Extremely Negative': -2,\n    'Negative': -1,\n    'Neutral': 0,\n    'Positive': 1,\n    'Extremely Positive': 2\n}\n\n# استخدام دالة map() لتحويل القيم النصية إلى القيم العددية\ndf1['Sentiment_1'] = df1['Sentiment'].map(mapping)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:17.547229Z","iopub.execute_input":"2024-04-13T12:59:17.547586Z","iopub.status.idle":"2024-04-13T12:59:17.558755Z","shell.execute_reply.started":"2024-04-13T12:59:17.547558Z","shell.execute_reply":"2024-04-13T12:59:17.557698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:19.811228Z","iopub.execute_input":"2024-04-13T12:59:19.811674Z","iopub.status.idle":"2024-04-13T12:59:19.82661Z","shell.execute_reply.started":"2024-04-13T12:59:19.811629Z","shell.execute_reply":"2024-04-13T12:59:19.825615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['Location'] = df1['Location'].apply(remove_special_characters)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:22.294694Z","iopub.execute_input":"2024-04-13T12:59:22.295116Z","iopub.status.idle":"2024-04-13T12:59:22.387003Z","shell.execute_reply.started":"2024-04-13T12:59:22.295085Z","shell.execute_reply":"2024-04-13T12:59:22.386048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# إنشاء مخطط الصندوق والشارب\nplt.figure(figsize=(6, 15))\nsns.boxplot(x='Sentiment_1', y='Location', data=df1[20:80])\nplt.xlabel('Category')\nplt.ylabel('Value')\nplt.title('Boxplot of Category vs Value')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:59:23.484739Z","iopub.execute_input":"2024-04-13T12:59:23.48512Z","iopub.status.idle":"2024-04-13T12:59:24.796419Z","shell.execute_reply.started":"2024-04-13T12:59:23.485091Z","shell.execute_reply":"2024-04-13T12:59:24.795402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df1['Sentiment']), len(df1['Location']))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:25.22078Z","iopub.execute_input":"2024-04-13T13:06:25.222073Z","iopub.status.idle":"2024-04-13T13:06:25.229017Z","shell.execute_reply.started":"2024-04-13T13:06:25.222029Z","shell.execute_reply":"2024-04-13T13:06:25.227879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# التحقق من القيم المفقودة في العمود\nmissing_values = df1['Location'].isnull()\n\n# استخدام العملية ~ لتحديد الصفوف التي لا تحتوي على قيم مفقودة\nvalid_values = ~missing_values\n\n# حذف الصفوف التي تحتوي على القيم الجغرافية\ndf1 = df1[valid_values] \n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:26.646477Z","iopub.execute_input":"2024-04-13T13:06:26.646922Z","iopub.status.idle":"2024-04-13T13:06:26.662782Z","shell.execute_reply.started":"2024-04-13T13:06:26.646881Z","shell.execute_reply":"2024-04-13T13:06:26.661643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# تحويل القيم في عمود البيانات إلى نصوص\ndf1['Location'] = df1['Location'].astype(str)\n\n# استخدام تعبير الاسترجاع النصي لتحديد الصفوف التي تحتوي على القيم الجغرافية\ngeographic_values = df1['Location'].str.contains(r'\\b\\d+\\.\\d+,-\\d+\\.\\d+\\b', regex=True)\n\n# حذف الصفوف التي تحتوي على القيم الجغرافية\ndf1 = df1[~geographic_values]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:28.167988Z","iopub.execute_input":"2024-04-13T13:06:28.16836Z","iopub.status.idle":"2024-04-13T13:06:28.212701Z","shell.execute_reply.started":"2024-04-13T13:06:28.168331Z","shell.execute_reply":"2024-04-13T13:06:28.211458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:29.900839Z","iopub.execute_input":"2024-04-13T13:06:29.901257Z","iopub.status.idle":"2024-04-13T13:06:29.918331Z","shell.execute_reply.started":"2024-04-13T13:06:29.901221Z","shell.execute_reply":"2024-04-13T13:06:29.917187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:31.115846Z","iopub.execute_input":"2024-04-13T13:06:31.116287Z","iopub.status.idle":"2024-04-13T13:06:31.130625Z","shell.execute_reply.started":"2024-04-13T13:06:31.116251Z","shell.execute_reply":"2024-04-13T13:06:31.12948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# قراءة البيانات إلى DataFrame\n# على سبيل المثال، لنفترض أن لديك DataFrame يسمى df والعمود الذي تريد تنظيفه هو \"Location\"\n\n# دالة لإزالة الرموز غير المرغوب فيها باستخدام مكتبة re\ndef remove_special_characters(text):\n    # تعريف التعبير العادي لإزالة الرموز\n    pattern = r'[^a-zA-Z\\s]'   # يتم الاحتفاظ بالأحرف الأبجدية الإنجليزية، الأرقام، والفراغات\n     \n    # استخدام الدالة sub() لإزالة الرموز باستخدام التعبير العادي\n    cleaned_text = re.sub(pattern, '', text)\n\n    return cleaned_text\n\n# تطبيق الدالة على عمود \"Location\"\ndf['Location'] = df['Location'].apply(remove_special_characters)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:32.37407Z","iopub.execute_input":"2024-04-13T13:06:32.374434Z","iopub.status.idle":"2024-04-13T13:06:32.389434Z","shell.execute_reply.started":"2024-04-13T13:06:32.374406Z","shell.execute_reply":"2024-04-13T13:06:32.388448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# تحميل قائمة الكلمات الوقفية\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\n\n# قائمة لتخزين النصوص المنظفة\ncleaned_texts = []\n\n# تنظيف النصوص\nfor text in df1['OriginalTweet']:\n    # تحويل النص إلى حروف صغيرة\n    text = text.lower()\n    \n    # إزالة الرموز الغير مفيدة\n    text = text.translate(str.maketrans('', '', string.punctuation + '@#$'))\n    # إزالة الروابط\n    text = re.sub(r'http\\S+', '', text)\n    # تقسيم النص إلى كلمات\n    words = word_tokenize(text)\n    \n    # إزالة الكلمات الوقفية\n    words = [word for word in words if word not in stop_words]\n    \n    # إعادة تجميع الكلمات لتكوين النص النظيف\n    cleaned_text = ' '.join(words)\n    \n    # إضافة النص النظيف إلى القائمة\n    cleaned_texts.append(cleaned_text)\n\n# إنشاء DataFrame جديد يحتوي على النصوص المنظفة\n\ndf1['OriginalTweet_cleaned'] = cleaned_texts\n\n# طباعة النتائج أو حفظها في ملف CSV\nprint(df1.head())\n\n# لحفظ البيانات المنظفة في ملف CSV\n# cleaned_df.to_csv('مسار_الملف_المنظف.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:33.681298Z","iopub.execute_input":"2024-04-13T13:06:33.681782Z","iopub.status.idle":"2024-04-13T13:06:46.548528Z","shell.execute_reply.started":"2024-04-13T13:06:33.681748Z","shell.execute_reply":"2024-04-13T13:06:46.547377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:07:36.165622Z","iopub.execute_input":"2024-04-13T13:07:36.166333Z","iopub.status.idle":"2024-04-13T13:07:36.182187Z","shell.execute_reply.started":"2024-04-13T13:07:36.166297Z","shell.execute_reply":"2024-04-13T13:07:36.18091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#32567\n#32567\n#32567\ndf1 = df1.reset_index()\ndf1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:07:38.373993Z","iopub.execute_input":"2024-04-13T13:07:38.374418Z","iopub.status.idle":"2024-04-13T13:07:38.400309Z","shell.execute_reply.started":"2024-04-13T13:07:38.374375Z","shell.execute_reply":"2024-04-13T13:07:38.399089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5,25):\n    print(f\"{i+1}: {df1['OriginalTweet_cleaned'][i]} -> {df1['Sentiment'][i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:07:40.771752Z","iopub.execute_input":"2024-04-13T13:07:40.7726Z","iopub.status.idle":"2024-04-13T13:07:40.778871Z","shell.execute_reply.started":"2024-04-13T13:07:40.772551Z","shell.execute_reply":"2024-04-13T13:07:40.777735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def choosingcol(df1):\n    new_df=df1.loc[:, ['OriginalTweet_cleaned', 'Sentiment']]\n    return new_df\n\n#------------------------------------------------------------------\ndef reducing_sentiment(df1):\n    df1 = df1.loc[df1['Sentiment'] != 'Neutral']\n\n    # replace extremely positive and extremely negative with positive and negative, respectively\n    df1['Sentiment'] = df1['Sentiment'].replace({'Extremely Positive': 'Positive', 'Extremely Negative': 'Negative'})\n    \n    return df1\n#-----------------------------------------------------------------\ndef converting_tonumeric(df):\n    sentiment_dict = {'Positive': 1, 'Negative': 0}\n    df1['Sentiment'] = df1['Sentiment'].replace(sentiment_dict)\n    return df1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:16:00.636392Z","iopub.execute_input":"2024-04-13T13:16:00.636936Z","iopub.status.idle":"2024-04-13T13:16:00.644875Z","shell.execute_reply.started":"2024-04-13T13:16:00.6369Z","shell.execute_reply":"2024-04-13T13:16:00.643776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using functions to make train_set ready \ndf1 = choosingcol(df1)\ndf1 = reducing_sentiment(df1)\ndf1 = converting_tonumeric(df1)\ndf1 = df1.reset_index(drop=True)\ndf1 ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:17:25.444445Z","iopub.execute_input":"2024-04-13T13:17:25.445227Z","iopub.status.idle":"2024-04-13T13:17:25.464211Z","shell.execute_reply.started":"2024-04-13T13:17:25.445191Z","shell.execute_reply":"2024-04-13T13:17:25.46308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contractions_dict = {\n    \"ain't\": \"am not / are not / is not / has not / have not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he had / he would\",\n    \"he'll\": \"he shall / he will\",\n    \"he's\": \"he is\",\n    \"I'd\": \"I had / I would\",\n    \"I'll\": \"I shall / I will\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it had / it would\",\n    \"it'll\": \"it shall / it will\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"shan't\": \"shall not\",\n    \"she'd\": \"she had / she would\",\n    \"she'll\": \"she shall / she will\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"that's\": \"that is\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they had / they would\",\n    \"they'll\": \"they shall / they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we had / we would\",\n    \"we'll\": \"we shall / we will\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what shall / what will\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"where's\": \"where is\",\n    \"who'd\": \"who had / who would\",\n    \"who'll\": \"who shall / who will\",\n    \"who're\": \"who are\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"won't\": \"will not\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"you'd\": \"you had / you would\",\n    \"you'll\": \"you shall / you will\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:17:39.448113Z","iopub.execute_input":"2024-04-13T13:17:39.448492Z","iopub.status.idle":"2024-04-13T13:17:39.460749Z","shell.execute_reply.started":"2024-04-13T13:17:39.448459Z","shell.execute_reply":"2024-04-13T13:17:39.45938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaning_with_re(tweet):\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n\n    # remove hyperlinks\n    tweet = re.sub(r'https?://(?:www\\.[^\\s\\n\\r]+|[^\\s\\n\\r]+)', '', tweet)\n\n    # remove hashtag (#)\n    tweet = re.sub(r'#', '', tweet)\n    \n    #replaces newline (\\n) and carriage return (\\r) characters in a tweet with an empty string\n    tweet = re.sub(r'[\\n\\r]', '', tweet)\n    \n    #replaces the numbers with an empty string\n    tweet = re.sub(r'\\d+', '', tweet)\n    \n    return tweet\n\n#----------------------------------------------------------------\ndef expanding_words(tweet):\n    \n    words = tweet.split()\n    expanded_words = []\n    for word in words:\n        if word.lower() in contractions_dict:\n            expanded_words.extend(contractions_dict[word.lower()].split(\"/\"))\n        else:\n            expanded_words.append(word)\n    return \" \".join(expanded_words)\n\n#----------------------------------------------------------------\ndef tokenizing(tweet):\n    \n    stopwords_english = stopwords.words('english') \n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n    new_tweet = tokenizer.tokenize(tweet)\n    \n    clean_tweet=[]\n    \n    for word in new_tweet: # Going through every word in tokens list\n        if (word not in stopwords_english and word not in string.punctuation):  # remove punctuation and stopwords\n            clean_tweet.append(word)\n\n    return clean_tweet\n\n#----------------------------------------------------------------\ndef stemming(tweet):\n    # Instantiate stemming class\n    stemmer = PorterStemmer() \n\n    # Create an empty list to store the stems\n    stemmed_tweets = [] \n\n    for word in tweet:\n        stem_word = stemmer.stem(word)  # stemming word\n        stemmed_tweets.append(stem_word)  # append to the list\n        \n    return stemmed_tweets","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:18:11.702301Z","iopub.execute_input":"2024-04-13T13:18:11.702676Z","iopub.status.idle":"2024-04-13T13:18:11.714326Z","shell.execute_reply.started":"2024-04-13T13:18:11.702631Z","shell.execute_reply":"2024-04-13T13:18:11.713169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(tweet):\n    \n    tweet=cleaning_with_re(tweet)\n    tweet=expanding_words(tweet)\n    tweet=tokenizing(tweet)\n    tweet=stemming(tweet)\n    \n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:18:16.722599Z","iopub.execute_input":"2024-04-13T13:18:16.723587Z","iopub.status.idle":"2024-04-13T13:18:16.728714Z","shell.execute_reply.started":"2024-04-13T13:18:16.723544Z","shell.execute_reply":"2024-04-13T13:18:16.727414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_freqs(listoftweets, labels):\n    \n    labelslist = np.squeeze(labels).tolist()\n    \n    # Start with an empty dictionary and populate it by looping over all tweets and all the words in them\n    freqs = {}\n    for y, tweet in zip(labels, listoftweets):\n        t=preprocessing(tweet)\n        for word in t:\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n                \n    # Calculate the sum of frequencies for each word\n    word_sums = {}\n    for (word, y), count in freqs.items():\n        if word in word_sums:\n            word_sums[word] += count\n        else:\n            word_sums[word] = count\n\n    # Divide each key of word and class by the sum\n    for (word, y), count in freqs.items():\n        freqs[(word, y)] /= word_sums[word]\n            \n    return freqs\n\n#------------------------------------------------------------------\ndef extract_features(tweet, freqs, preprocessing=preprocessing):\n    \n    # process_tweet tokenizes, stems, and removes stopwords\n    word_list = preprocessing(tweet)\n    \n    # 2 elements for [ positive, negative] counts\n    features = np.zeros(2) \n\n    # loop through each word in the list of words\n    for word in word_list:\n        \n        # increment the word count for the positive label 1\n        features[0] += freqs.get((word, 1.0),0)\n            \n        # increment the word count for the negative label 0\n        features[1] += freqs.get((word, 0),0)\n    \n    features = features[None, :] \n    assert(features.shape == (1, 2))\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:18:18.019181Z","iopub.execute_input":"2024-04-13T13:18:18.020172Z","iopub.status.idle":"2024-04-13T13:18:18.031454Z","shell.execute_reply.started":"2024-04-13T13:18:18.020136Z","shell.execute_reply":"2024-04-13T13:18:18.030534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nword_frequencies=build_freqs(df1['OriginalTweet_cleaned'].tolist(), df1['Sentiment'].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:18:19.929841Z","iopub.execute_input":"2024-04-13T13:18:19.93088Z","iopub.status.idle":"2024-04-13T13:18:54.27963Z","shell.execute_reply.started":"2024-04-13T13:18:19.930831Z","shell.execute_reply":"2024-04-13T13:18:54.278499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.zeros((len(df1['Sentiment']), 2))\nfor i in range(len(df1['Sentiment'])):\n    X[i, :]= extract_features(df1['OriginalTweet_cleaned'][i], word_frequencies)\n\nprint(X)\n    \n# training labels corresponding to X\nY = np.array(df1['Sentiment'])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:19:01.71934Z","iopub.execute_input":"2024-04-13T13:19:01.719791Z","iopub.status.idle":"2024-04-13T13:19:38.344414Z","shell.execute_reply.started":"2024-04-13T13:19:01.719755Z","shell.execute_reply":"2024-04-13T13:19:38.343411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:23:17.063923Z","iopub.execute_input":"2024-04-13T13:23:17.064358Z","iopub.status.idle":"2024-04-13T13:23:17.071931Z","shell.execute_reply.started":"2024-04-13T13:23:17.064328Z","shell.execute_reply":"2024-04-13T13:23:17.070735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:21:01.703167Z","iopub.execute_input":"2024-04-13T13:21:01.704043Z","iopub.status.idle":"2024-04-13T13:21:01.709145Z","shell.execute_reply.started":"2024-04-13T13:21:01.704Z","shell.execute_reply":"2024-04-13T13:21:01.707989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model \nmodel = Sequential()\nmodel.add(Dense(4, input_dim=2, activation='relu'))  # Hidden layer with 4 neurons\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))  # Output layer with logistic regression\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=11, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T14:06:47.598024Z","iopub.execute_input":"2024-04-13T14:06:47.598457Z","iopub.status.idle":"2024-04-13T14:07:02.314537Z","shell.execute_reply.started":"2024-04-13T14:06:47.598423Z","shell.execute_reply":"2024-04-13T14:07:02.313436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df1.OriginalTweet_cleaned)\n\n# عدد الكلمات الفريدة\nnum_unique_words = len(tokenizer.word_index)\nprint(\"Number of unique words:\", num_unique_words)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:25:38.632495Z","iopub.execute_input":"2024-04-13T01:25:38.63286Z","iopub.status.idle":"2024-04-13T01:25:39.855088Z","shell.execute_reply.started":"2024-04-13T01:25:38.632833Z","shell.execute_reply":"2024-04-13T01:25:39.853998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# إضافة عمود جديد يحتوي على طول كل جملة\ndf1['sentence_length'] = df1['OriginalTweet_cleaned'].apply(lambda x: len(x.split()))\n\n# طباعة أول عشر صفوف للتحقق\ndf1.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:31:37.130572Z","iopub.execute_input":"2024-04-13T01:31:37.130997Z","iopub.status.idle":"2024-04-13T01:31:37.236196Z","shell.execute_reply.started":"2024-04-13T01:31:37.130967Z","shell.execute_reply":"2024-04-13T01:31:37.234937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:34:47.704979Z","iopub.execute_input":"2024-04-13T01:34:47.705499Z","iopub.status.idle":"2024-04-13T01:34:47.741618Z","shell.execute_reply.started":"2024-04-13T01:34:47.705463Z","shell.execute_reply":"2024-04-13T01:34:47.740002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 54228  \nmax_length = 100 \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf1['Sentiment'] = le.fit_transform(df1['Sentiment'])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T02:02:57.774573Z","iopub.execute_input":"2024-04-13T02:02:57.775136Z","iopub.status.idle":"2024-04-13T02:02:57.781565Z","shell.execute_reply.started":"2024-04-13T02:02:57.775093Z","shell.execute_reply":"2024-04-13T02:02:57.780309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# تقسيم البيانات إلى مجموعات الاختبار والتدريب\n#X_train, X_test, y_train, y_test = train_test_split(df1['OriginalTweet_cleaned'], df1['Sentiment'], test_size=0.2, random_state=42)\n\n# مثال على تحويل النص إلى متغيرات قابلة للتحليل\n#vectorizer = CountVectorizer()\n#X_train_vectorized = vectorizer.fit_transform(X_train)\n#X_test_vectorized = vectorizer.transform(X_test)\n\n# بناء وتدريب النموذج\n#model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n#model.fit(X_train_vectorized, y_train)\n\n# التنبؤ باستخدام النموذج\n#predictions = model.predict(X_test_vectorized)\n\n# قياس أداء النموذج\n#accuracy = accuracy_score(y_test, predictions)\n#print(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:36:41.715885Z","iopub.execute_input":"2024-04-10T14:36:41.716352Z","iopub.status.idle":"2024-04-10T15:05:53.417881Z","shell.execute_reply.started":"2024-04-10T14:36:41.716321Z","shell.execute_reply":"2024-04-10T15:05:53.414343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dim = len(df1['Sentiment'].unique())  # Assuming 'label' is the column containing class labels\noutput_dim","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:42:31.480099Z","iopub.execute_input":"2024-04-11T14:42:31.480536Z","iopub.status.idle":"2024-04-11T14:42:31.49232Z","shell.execute_reply.started":"2024-04-11T14:42:31.480501Z","shell.execute_reply":"2024-04-11T14:42:31.49094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.preprocessing import LabelEncoder\n#lb = LabelEncoder()\n#df1.Sentiment = lb.fit_transform(df1.Sentiment)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:41:07.573551Z","iopub.execute_input":"2024-04-11T16:41:07.574013Z","iopub.status.idle":"2024-04-11T16:41:07.591232Z","shell.execute_reply.started":"2024-04-11T16:41:07.573982Z","shell.execute_reply":"2024-04-11T16:41:07.589751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x = df1.drop(['index','UserName','Location','TweetAt','OriginalTweet','Sentiment_1','Sentiment','ScreenName'],axis=1)\nx = df1['OriginalTweet_cleaned']\ny = df1['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:41:12.858819Z","iopub.execute_input":"2024-04-11T16:41:12.859319Z","iopub.status.idle":"2024-04-11T16:41:12.865145Z","shell.execute_reply.started":"2024-04-11T16:41:12.859281Z","shell.execute_reply":"2024-04-11T16:41:12.863657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape\nvocab_size = 10000  \nmax_length = 100","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:12:47.946934Z","iopub.execute_input":"2024-04-11T16:12:47.947371Z","iopub.status.idle":"2024-04-11T16:12:47.955502Z","shell.execute_reply.started":"2024-04-11T16:12:47.947339Z","shell.execute_reply":"2024-04-11T16:12:47.954117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T20:59:51.110332Z","iopub.execute_input":"2024-04-11T20:59:51.110776Z","iopub.status.idle":"2024-04-11T20:59:51.116753Z","shell.execute_reply.started":"2024-04-11T20:59:51.110745Z","shell.execute_reply":"2024-04-11T20:59:51.115062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}